{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_dir, prefix, samples=None):\n",
    "    # Define file paths\n",
    "    node_file = os.path.join(input_dir, f'{prefix}.node.csv')\n",
    "    edge_file = os.path.join(input_dir, f'{prefix}.edge.csv')\n",
    "    path_file = os.path.join(input_dir, f'{prefix}.path.label.csv')\n",
    "    phasing_file = os.path.join(input_dir, f'{prefix}.phasing.csv')\n",
    "    \n",
    "    # Define dtypes\n",
    "    node_dtypes = {\"chr\": str, \"graph_id\": str, \"node_id\": int, \"start_pos\": np.uint64, \"end_pos\": np.uint64, \n",
    "                   \"weight\": float, \"length\": int, \"maxcov\": float, \"stddev\": float, \"indel_sum_cov\": float, \n",
    "                   \"indel_ratio\": float, \"left_indel\": int, \"right_indel\": int, \"sample\": str}\n",
    "    edge_dtypes = {\"chr\": str, \"graph_id\": str, \"source\": int, \"target\": int, \"start_pos\": np.uint64, \n",
    "                   \"end_pos\": np.uint64, \"weight\": float, \"length\": int, \"sample\": str}\n",
    "    path_dtypes = {\"chr\": str, \"graph_id\": str, \"path_id\": str, \"node_sequence\": str, \"splice_source\": str, \n",
    "                   \"splice_target\": str, \"abundance\": float, \"label\": int, \"sample\": str}\n",
    "    phasing_dtypes = {\"chr\": str, \"graph_id\": str, \"path_id\": str, \"node_sequence\": str, \"count\": int, \"sample\": str}\n",
    "    \n",
    "    # Read CSV files with headers\n",
    "    node_df = pd.read_csv(node_file, dtype=node_dtypes)\n",
    "    edge_df = pd.read_csv(edge_file, dtype=edge_dtypes)\n",
    "    path_df = pd.read_csv(path_file, dtype=path_dtypes)\n",
    "    phasing_df = pd.read_csv(phasing_file, dtype=phasing_dtypes)\n",
    "    \n",
    "    # Filter by samples if specified\n",
    "    if samples:\n",
    "        node_df = node_df[node_df['sample'].isin(samples)]\n",
    "        edge_df = edge_df[edge_df['sample'].isin(samples)]\n",
    "        path_df = path_df[path_df['sample'].isin(samples)]\n",
    "        phasing_df = phasing_df[phasing_df['sample'].isin(samples)]\n",
    "    \n",
    "    return node_df, edge_df, path_df, phasing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['polyester_test1_refseq_1']\n",
    "node_df, edge_df, path_df, phasing_df = load_data('/data/qzs23/projects/pathEm/aletsch-results/nnInput', 'polyester_refseq.full', samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m node_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m edge_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m path_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'node_df' is not defined"
     ]
    }
   ],
   "source": [
    "# node_df.to_csv('node_df.csv')\n",
    "# edge_df.to_csv('edge_df.csv')\n",
    "# path_df.to_csv('path_df.csv')\n",
    "# phasing_df.to_csv('phasing_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_graph_to_input_data(node_df, edge_df, phasing_df, graph_id):\n",
    "\n",
    "    nodes = node_df[node_df['graph_id'] == graph_id]\n",
    "    edges = edge_df[edge_df['graph_id'] == graph_id]\n",
    "    phasing = phasing_df[phasing_df['graph_id'] == graph_id]\n",
    "    \n",
    "    if len(nodes) < 5:\n",
    "        return None\n",
    "    if len(edges) == 0:\n",
    "        return None\n",
    "\n",
    "    node_feature_cols = ['extPathSupport', 'weight', 'length', 'maxcov', 'stddev', 'indel_sum_cov','indel_ratio','left_indel','right_indel']\n",
    "    edge_feature_cols = ['extPathSupport', 'weight', 'length']\n",
    "\n",
    "    # Define torch tensors with custom features for nodes and edges\n",
    "    node_features = torch.tensor(nodes[node_feature_cols].values, dtype=torch.float)\n",
    "    edge_features = torch.tensor(edges[edge_feature_cols].values, dtype=torch.float)\n",
    "\n",
    "    # Tensor for the Data object to maintain links\n",
    "    edge_index = torch.tensor(edges[['source', 'target']].values.T, dtype=torch.long)\n",
    "\n",
    "    num_nodes = len(nodes)\n",
    "    num_edges = len(edges)\n",
    "\n",
    "    # Initialize coverage with zeros\n",
    "    node_phasing_coverage = torch.zeros(num_nodes, dtype=torch.float)\n",
    "    edge_phasing_coverage = torch.zeros(num_edges, dtype=torch.float)\n",
    "\n",
    "    # Build a lookup from node_id -> row index in 'nodes'\n",
    "    node_ids = nodes['node_id'].values\n",
    "    node_index_lookup = {nid: i for i, nid in enumerate(node_ids)}\n",
    "\n",
    "    # Build a lookup for (source, target) -> row index in 'edges'\n",
    "    edge_lookup = {}\n",
    "    for i, e_row in edges.iterrows():\n",
    "        s, t = e_row['source'], e_row['target']\n",
    "        edge_lookup[(s, t)] = i\n",
    "\n",
    "    # Aggregate phasing coverage\n",
    "    for _, row in phasing.iterrows():\n",
    "        node_seq_str = row.get('node_sequence', None)\n",
    "        if not isinstance(node_seq_str, str):\n",
    "            continue  # skip invalid or empty sequences\n",
    "        \n",
    "        count_value = row.get('count', 0)\n",
    "        node_list = list(map(int, node_seq_str.split(',')))  # e.g. \"0,1,2\" -> [0,1,2]\n",
    "\n",
    "        # Update node coverage\n",
    "        for nid in node_list:\n",
    "            if nid in node_index_lookup:\n",
    "                node_phasing_coverage[node_index_lookup[nid]] += count_value\n",
    "\n",
    "        # Update edge coverage (for consecutive pairs)\n",
    "        for s, t in zip(node_list, node_list[1:]):\n",
    "            if (s, t) in edge_lookup:\n",
    "                edge_idx = edge_lookup[(s, t)]\n",
    "                edge_phasing_coverage[edge_idx] += count_value\n",
    "\n",
    "    node_phasing_coverage = node_phasing_coverage.view(-1, 1)  # shape [num_nodes, 1]\n",
    "    edge_phasing_coverage = edge_phasing_coverage.view(-1, 1)  # shape [num_edges, 1]\n",
    "\n",
    "    node_features = torch.cat([node_features, node_phasing_coverage], dim=1)\n",
    "    edge_features = torch.cat([edge_features, edge_phasing_coverage], dim=1)\n",
    "\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_features\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = process_input_graph_to_input_data(node_df=node_df, edge_df=edge_df, phasing_df=phasing_df, graph_id='chrNC_000001.11.instance.0.0.2.0.0.polyester_test1_refseq_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GNNForRLAgent(nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_dim, num_heads, num_gat_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Normalize input node and edge features\n",
    "        self.input_norm = nn.BatchNorm1d(num_node_features)\n",
    "        self.edge_norm = nn.BatchNorm1d(num_edge_features)\n",
    "\n",
    "        # Define GAT layers. The first layer converts the input node features into a hidden space.\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.gat_layers.append(\n",
    "            GATConv(num_node_features, hidden_dim, heads=num_heads, edge_dim=num_edge_features)\n",
    "        )\n",
    "        # For subsequent layers, the input dimension is (hidden_dim * num_heads)\n",
    "        for _ in range(num_gat_layers - 1):\n",
    "            self.gat_layers.append(\n",
    "                GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, edge_dim=num_edge_features)\n",
    "            )\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim * num_heads + num_edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # Normalize the features\n",
    "        x = self.input_norm(x)\n",
    "        edge_attr = self.edge_norm(edge_attr)\n",
    "\n",
    "        # Pass through the GAT layers\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = gat_layer(x, edge_index, edge_attr)\n",
    "            x = F.elu(x)  # non-linear activation\n",
    "\n",
    "        # Now, x has shape [num_nodes, hidden_dim * num_heads]\n",
    "        # Get the embeddings for the source and target nodes for every edge.\n",
    "        source_nodes = edge_index[0]  # [num_edges]\n",
    "        target_nodes = edge_index[1]  # [num_edges]\n",
    "        h_source = x[source_nodes]    # [num_edges, hidden_dim * num_heads]\n",
    "        h_target = x[target_nodes]    # [num_edges, hidden_dim * num_heads]\n",
    "\n",
    "        # Concatenate the source and target embeddings with the edge features.\n",
    "        edge_input = torch.cat([h_source, h_target, edge_attr], dim=-1)\n",
    "        # edge_input now has shape: [num_edges, 2*hidden_dim*num_heads + num_edge_features]\n",
    "\n",
    "        # Pass through the final MLP to produce edge logits.\n",
    "        edge_logits = self.edge_mlp(edge_input).squeeze(-1)  # [num_edges]\n",
    "\n",
    "        # The output logits can be used directly in the RL agent for masking and computing softmax.\n",
    "        return edge_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debug Forward Pass ===\n",
      "Input node features shape: torch.Size([83, 10])\n",
      "Input edge features shape: torch.Size([81, 4])\n",
      "Input edge index shape: torch.Size([2, 81])\n",
      "--------------------------------------------------\n",
      "After normalization:\n",
      "Node features shape: torch.Size([83, 10])\n",
      "Edge features shape: torch.Size([81, 4])\n",
      "--------------------------------------------------\n",
      "After GAT layer 1:\n",
      "Node features shape: torch.Size([83, 256])\n",
      "--------------------------------------------------\n",
      "After GAT layer 2:\n",
      "Node features shape: torch.Size([83, 256])\n",
      "--------------------------------------------------\n",
      "After GAT layer 3:\n",
      "Node features shape: torch.Size([83, 256])\n",
      "--------------------------------------------------\n",
      "Source node embeddings shape: torch.Size([81, 256])\n",
      "Target node embeddings shape: torch.Size([81, 256])\n",
      "--------------------------------------------------\n",
      "Concatenated edge input shape: torch.Size([81, 516])\n",
      "--------------------------------------------------\n",
      "Final edge logits shape: torch.Size([81])\n",
      "Edge logits: tensor([ 3.9506e+00,  3.3674e+00,  1.9325e+00, -9.6862e-01,  1.5001e+00,\n",
      "         2.1405e+00,  2.4986e+00,  1.8257e+00, -7.9340e+01,  3.9923e+00,\n",
      "        -2.0308e+00,  5.5581e-01,  1.7298e+00,  2.0859e+00,  1.3441e+00,\n",
      "         6.9091e+00,  1.3831e+00,  4.6134e+00,  9.1546e+00,  1.1965e+00,\n",
      "         4.0882e+00, -7.6623e-02,  9.3887e-01,  6.1486e+00,  2.4298e+00,\n",
      "         2.6237e+00, -1.0293e+00, -9.7176e+01, -2.1619e+02, -5.1167e+01,\n",
      "        -1.3966e+02,  9.1506e-01,  7.0649e+00, -1.8033e+00,  3.9854e+00,\n",
      "         3.7192e+00,  6.0026e-01,  6.2700e-01, -1.7372e+02,  1.4382e+00,\n",
      "         9.7034e-01,  1.6355e+00,  3.5912e-01,  2.0616e+00, -9.2244e+00,\n",
      "        -2.8566e+02,  7.5042e+00,  7.2158e-01,  9.1744e-02,  2.1552e+00,\n",
      "         1.5221e+00, -2.3912e+02,  7.0024e-01,  1.9933e+00,  6.3758e+00,\n",
      "        -4.9622e-01, -5.4521e-01, -2.7210e+00, -1.4784e+02, -6.0499e-01,\n",
      "         1.4829e-03, -1.0996e+01,  5.1145e-01,  3.6178e-01,  5.1696e-01,\n",
      "         1.0273e+00, -8.1628e-02,  2.9058e+00, -2.6358e+02, -6.7258e+01,\n",
      "         2.1500e+00, -3.3783e+00,  8.7532e-01, -7.7917e+01,  5.6099e-01,\n",
      "         2.4589e+00,  1.5685e+00, -6.0480e-01,  3.9403e-01,  1.2503e+00,\n",
      "        -2.1707e+02], grad_fn=<SqueezeBackward1>)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "num_node_features = data_obj.x.size(1)\n",
    "num_edge_features = data_obj.edge_attr.size(1)\n",
    "\n",
    "hidden_dim = 64   \n",
    "num_heads = 4     \n",
    "num_gat_layers = 3  \n",
    "\n",
    "model = GNNForRLAgent(num_node_features, num_edge_features, hidden_dim, num_heads, num_gat_layers)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def debug_forward(model, data):\n",
    "    print(\"=== Debug Forward Pass ===\")\n",
    "    print(\"Input node features shape:\", data.x.shape)\n",
    "    print(\"Input edge features shape:\", data.edge_attr.shape)\n",
    "    print(\"Input edge index shape:\", data.edge_index.shape)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Normalize features\n",
    "    x = model.input_norm(data.x)\n",
    "    edge_attr = model.edge_norm(data.edge_attr)\n",
    "    print(\"After normalization:\")\n",
    "    print(\"Node features shape:\", x.shape)\n",
    "    print(\"Edge features shape:\", edge_attr.shape)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Pass through each GAT layer with activation\n",
    "    for i, gat_layer in enumerate(model.gat_layers):\n",
    "        x = gat_layer(x, data.edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        print(f\"After GAT layer {i + 1}:\")\n",
    "        print(\"Node features shape:\", x.shape)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Extract node embeddings for each edge\n",
    "    source_nodes = data.edge_index[0]\n",
    "    target_nodes = data.edge_index[1]\n",
    "    h_source = x[source_nodes]\n",
    "    h_target = x[target_nodes]\n",
    "    print(\"Source node embeddings shape:\", h_source.shape)\n",
    "    print(\"Target node embeddings shape:\", h_target.shape)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Concatenate source and target embeddings with edge features\n",
    "    edge_input = torch.cat([h_source, h_target, edge_attr], dim=-1)\n",
    "    print(\"Concatenated edge input shape:\", edge_input.shape)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compute edge logits via the final MLP\n",
    "    edge_logits = model.edge_mlp(edge_input).squeeze(-1)\n",
    "    print(\"Final edge logits shape:\", edge_logits.shape)\n",
    "    print(\"Edge logits:\", edge_logits)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return edge_logits\n",
    "\n",
    "# Run the debug-forward pass on your already-loaded data object\n",
    "edge_logits = debug_forward(model, data_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpliceGraphEnv:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Data,\n",
    "        start_node: int,\n",
    "        end_node: int,\n",
    "        ground_truth_path: list,\n",
    "        max_steps: int = 50,\n",
    "        step_penalty: float = -0.01\n",
    "    ):\n",
    "       \n",
    "        self.data = data\n",
    "        self.start_node = start_node\n",
    "        self.end_node = end_node\n",
    "        self.ground_truth_path = ground_truth_path  \n",
    "        self.max_steps = max_steps\n",
    "        self.step_penalty = step_penalty\n",
    "\n",
    "        self.edge_index = data.edge_index\n",
    "        self.num_edges = self.edge_index.shape[1]\n",
    "\n",
    "        self.current_node = None\n",
    "        self.visited_edges = None\n",
    "        self.steps_taken = 0\n",
    "        self.done = False\n",
    "        self.path_history = []  \n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_node = self.start_node\n",
    "        self.visited_edges = set()\n",
    "        self.steps_taken = 0\n",
    "        self.done = False\n",
    "        self.path_history = []\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            \"current_node\": self.current_node,\n",
    "            \"visited_edges\": self.visited_edges,\n",
    "            \"steps_taken\": self.steps_taken,\n",
    "            \"data\": self.data\n",
    "        }\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        valid_edges = []\n",
    "        src_nodes = self.edge_index[0]  \n",
    "        for e_idx in range(self.num_edges):\n",
    "            if src_nodes[e_idx].item() == self.current_node and e_idx not in self.visited_edges:\n",
    "                valid_edges.append(e_idx)\n",
    "        return valid_edges\n",
    "\n",
    "    def step(self, action_edge_idx):\n",
    "        # If the edge is already visited or is invalid, we can penalize and end the episode\n",
    "        valid_actions = self.get_valid_actions()\n",
    "        if action_edge_idx not in valid_actions:\n",
    "            # Invalid action chosen\n",
    "            reward = -1.0\n",
    "            self.done = True\n",
    "            next_state = self.get_state()\n",
    "            return next_state, reward, self.done, {}\n",
    "\n",
    "        # Mark edge as visited\n",
    "        self.visited_edges.add(action_edge_idx)\n",
    "        self.path_history.append(action_edge_idx)\n",
    "\n",
    "        # Move to next node\n",
    "        dst_node = self.edge_index[1][action_edge_idx].item()\n",
    "        self.current_node = dst_node\n",
    "\n",
    "        # Step penalty\n",
    "        reward = self.step_penalty\n",
    "        self.steps_taken += 1\n",
    "\n",
    "        # Check if we reached the end node\n",
    "        if self.current_node == self.end_node:\n",
    "            # Compare assembled path to the ground truth\n",
    "            final_transcript_reward = self._evaluate_transcript()\n",
    "            reward += final_transcript_reward\n",
    "            self.done = True\n",
    "\n",
    "        # Check if maximum steps exceeded\n",
    "        if self.steps_taken >= self.max_steps:\n",
    "            # If we haven't reached end_node by now, we can finalize reward as well\n",
    "            if not self.done:\n",
    "                # Possibly some penalty for not finishing\n",
    "                reward -= 0.5\n",
    "                self.done = True\n",
    "\n",
    "        next_state = self.get_state()\n",
    "        return next_state, reward, self.done, {}\n",
    "\n",
    "    def _evaluate_transcript(self):\n",
    "        # TODO: If the nodes or edges match exactly, big reward.\n",
    "        # Otherwise, partial match or some scoring. We assume ground_truth_path\n",
    "        # is a list of edges for simplicity. If it's a list of nodes, adapt accordingly.\n",
    "        if self.path_history == self.ground_truth_path:\n",
    "            return 10.0  # perfect match reward\n",
    "        else:\n",
    "            return -0.2  # small negative if not perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        gnn_model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        gamma: float = 0.99\n",
    "    ):\n",
    "        self.model = gnn_model\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        data = state[\"data\"]\n",
    "        edge_logits = self.model(data)  \n",
    "\n",
    "        # Create a mask for valid actions\n",
    "        # valid_mask is 1 for valid edges, 0 for invalid\n",
    "        num_edges = edge_logits.shape[0]\n",
    "        mask = torch.zeros(num_edges, dtype=torch.bool, device=edge_logits.device)\n",
    "        mask[valid_actions] = True\n",
    "\n",
    "        # Large negative for invalid edges to effectively remove them from the softmax\n",
    "        masked_logits = torch.where(mask, edge_logits, torch.tensor(float('-inf'), device=edge_logits.device))\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(masked_logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action_edge_idx = dist.sample()\n",
    "        log_prob = dist.log_prob(action_edge_idx)\n",
    "\n",
    "        return action_edge_idx.item(), log_prob\n",
    "\n",
    "    def update_policy(self, trajectory):\n",
    "        \"\"\"\n",
    "        Implements REINFORCE update.\n",
    "        trajectory is a list of tuples:\n",
    "            [(state, action, reward, log_prob), ..., (terminal_state, None, ...)]\n",
    "        We first compute discounted returns, then compute the policy gradient loss.\n",
    "        \"\"\"\n",
    "        rewards = [tr[2] for tr in trajectory]\n",
    "        log_probs = [tr[3] for tr in trajectory if tr[3] is not None]\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Compute policy gradient loss\n",
    "        # Sum of (-log_prob * return)\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env: SpliceGraphEnv, agent: RLAgent, num_episodes: int = 500):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        # Storage for trajectory\n",
    "        # (state, action, reward, log_prob)\n",
    "        trajectory = []\n",
    "\n",
    "        while True:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "\n",
    "            # Edge case: if no valid actions remain, break\n",
    "            if len(valid_actions) == 0:\n",
    "                _, reward, done, _ = env.step(-1)  # triggers invalid action\n",
    "                # Add final step to trajectory\n",
    "                trajectory.append((state, -1, reward, None))\n",
    "                break\n",
    "\n",
    "            # Agent selects an action\n",
    "            action_edge_idx, log_prob = agent.select_action(state, valid_actions)\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action_edge_idx)\n",
    "\n",
    "            # Store in trajectory\n",
    "            trajectory.append((state, action_edge_idx, reward, log_prob))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update policy at the end of the episode\n",
    "        agent.update_policy(trajectory)\n",
    "\n",
    "        # Logging / print out\n",
    "        total_reward = sum([x[2] for x in trajectory])\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward:.3f}\")\n",
    "\n",
    "    print(\"Trainng done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: start_node =  \n",
    "# TODO: end_node = \n",
    "# TODO: Provide a ground_truth_path (list of edges or nodes) used for reward shaping:\n",
    "\n",
    "# TODO: env = SpliceGraphEnv(\n",
    "#     data=data_obj,\n",
    "#     start_node=start_node,\n",
    "#     end_node=end_node,\n",
    "#     ground_truth_path=ground_truth_path,\n",
    "#     max_steps=100  # or any suitable limit\n",
    "# )\n",
    "\n",
    "num_node_features = data_obj.x.size(-1)\n",
    "num_edge_features = data_obj.edge_attr.size(-1)\n",
    "\n",
    "gnn_model = GNNForRLAgent(\n",
    "    num_node_features=num_node_features,\n",
    "    num_edge_features=num_edge_features,\n",
    "    hidden_dim=64,  \n",
    "    num_heads=4,\n",
    "    num_gat_layers=3\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(gnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "agent = RLAgent(\n",
    "    gnn_model=gnn_model,\n",
    "    optimizer=optimizer,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "train_agent(env, agent, num_episodes=50)\n",
    "\n",
    "def assemble_transcript(env, agent):\n",
    "    \"\"\"\n",
    "    Runs the environment from start to end using the trained agent,\n",
    "    returns the list of edges visited and the corresponding node path.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            env.step(-1)\n",
    "            break\n",
    "\n",
    "        action_idx, _ = agent.select_action(state, valid_actions)\n",
    "        next_state, reward, done, _ = env.step(action_idx)\n",
    "        state = next_state\n",
    "\n",
    "    # The environment tracks visited edges in env.path_history.\n",
    "    node_path = [env.start_node]\n",
    "    for edge_idx in env.path_history:\n",
    "        dst_node = env.edge_index[1][edge_idx].item()\n",
    "        node_path.append(dst_node)\n",
    "\n",
    "    return env.path_history, node_path\n",
    "\n",
    "final_edge_path, final_node_path = assemble_transcript(env, agent)\n",
    "\n",
    "print(\"Assembled transcript (edge indices):\", final_edge_path)\n",
    "print(\"Assembled transcript (node sequence):\", final_node_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
